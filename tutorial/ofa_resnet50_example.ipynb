{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Successfully imported all packages and configured random seed to 1!\n"
     ]
    }
   ],
   "source": [
    "# build ofa resnet50\n",
    "from ofa.model_zoo import ofa_net\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "#from matplotlib import pyplot as plt\n",
    "from ofa.nas.search_algorithm import EvolutionFinder\n",
    "\n",
    "\n",
    "ofa_network = ofa_net('ofa_resnet50', pretrained=True)\n",
    "# set random seed\n",
    "random_seed = 1\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "print('Successfully imported all packages and configured random seed to %d!'%random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded checkpoint from /home/panyj/.ofa/ofa_resnet50_acc_predictor.pth\nThe accuracy predictor is ready!\nAccuracyPredictor(\n  (layers): Sequential(\n    (0): Sequential(\n      (0): Linear(in_features=82, out_features=400, bias=True)\n      (1): ReLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): Linear(in_features=400, out_features=400, bias=True)\n      (1): ReLU(inplace=True)\n    )\n    (2): Sequential(\n      (0): Linear(in_features=400, out_features=400, bias=True)\n      (1): ReLU(inplace=True)\n    )\n    (3): Linear(in_features=400, out_features=1, bias=False)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "# accuracy predictor\n",
    "import torch\n",
    "from ofa.nas.accuracy_predictor import AccuracyPredictor, ResNetArchEncoder\n",
    "from ofa.utils import download_url\n",
    "\n",
    "image_size_list = [128, 144, 160, 176, 192, 224, 240, 256]\n",
    "arch_encoder = ResNetArchEncoder(\n",
    "\timage_size_list=image_size_list, depth_list=ofa_network.depth_list, expand_list=ofa_network.expand_ratio_list,\n",
    "    width_mult_list=ofa_network.width_mult_list, base_depth_list=ofa_network.BASE_DEPTH_LIST\n",
    ")\n",
    "\n",
    "acc_predictor_checkpoint_path = download_url(\n",
    "    'https://hanlab.mit.edu/files/OnceForAll/tutorial/ofa_resnet50_acc_predictor.pth',\n",
    "    model_dir='~/.ofa/',\n",
    ")\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "acc_predictor = AccuracyPredictor(arch_encoder, 400, 3,\n",
    "                                  checkpoint_path=acc_predictor_checkpoint_path, device=device)\n",
    "\n",
    "print('The accuracy predictor is ready!')\n",
    "print(acc_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ofa.nas.memory_predictor'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-361f2c23c8bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mefficiency_predictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet50FLOPsModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mofa_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mofa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_predictor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResNet50WorkingMemModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmemory_predictor_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet50WorkingMemModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mofa_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmemory_predictor_ideal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet50WorkingMemModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mofa_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ofa.nas.memory_predictor'"
     ]
    }
   ],
   "source": [
    "# build efficiency predictor\n",
    "from ofa.nas.efficiency_predictor import ResNet50FLOPsModel\n",
    "\n",
    "efficiency_predictor = ResNet50FLOPsModel(ofa_network)\n",
    "\n",
    "from ofa.nas.memory_predictor import ResNet50WorkingMemModel \n",
    "memory_predictor_baseline = ResNet50WorkingMemModel(ofa_network, 0)\n",
    "memory_predictor_ideal = ResNet50WorkingMemModel(ofa_network, 1)\n",
    "memory_predictor_self = ResNet50WorkingMemModel(ofa_network, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ImageNet dataset files are ready.\n"
     ]
    }
   ],
   "source": [
    "# path to the ImageNet dataset\n",
    "imagenet_data_path = '/data2/jiecaoyu/imagenet/imgs/'\n",
    "print('The ImageNet dataset files are ready.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ImageNet dataloader is ready.\n"
     ]
    }
   ],
   "source": [
    "# The following function build the data transforms for test\n",
    "def build_val_transform(size):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(int(math.ceil(size / 0.875))),\n",
    "        transforms.CenterCrop(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(\n",
    "        root=os.path.join(imagenet_data_path, 'val'),\n",
    "        transform=build_val_transform(224)\n",
    "    ),\n",
    "    batch_size=250,  # test batch size\n",
    "    shuffle=True,\n",
    "    num_workers=12,  # number of workers for the data loader\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "print('The ImageNet dataloader is ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate random population...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching with constraint (5000,400):   0%|          | 0/200 [00:00<?, ?it/s, acc=0.795]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Evolution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching with constraint (5000,400): 100%|██████████| 200/200 [00:49<00:00,  4.03it/s, acc=0.802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found best architecture with FLOPS <= 5000.00 M and working mem <= 400.00 in 53.39 seconds! It achieves 80.24% predicted accuracy \n",
      "Architecture of the searched sub-net:\n",
      "DyConv(O24, K3, S2)\n",
      "(DyConv(O24, K3, S1), Identity)\n",
      "DyConv(O64, K3, S1)\n",
      "max_pooling(ks=3, stride=2)\n",
      "(3x3_BottleneckConv_in->56->168_S1, avgpool_conv)\n",
      "(3x3_BottleneckConv_in->56->168_S1, Identity)\n",
      "(3x3_BottleneckConv_in->56->168_S1, Identity)\n",
      "(3x3_BottleneckConv_in->56->168_S1, Identity)\n",
      "(3x3_BottleneckConv_in->176->512_S2, avgpool_conv)\n",
      "(3x3_BottleneckConv_in->176->512_S1, Identity)\n",
      "(3x3_BottleneckConv_in->128->512_S1, Identity)\n",
      "(3x3_BottleneckConv_in->176->512_S1, Identity)\n",
      "(3x3_BottleneckConv_in->360->1024_S2, avgpool_conv)\n",
      "(3x3_BottleneckConv_in->360->1024_S1, Identity)\n",
      "(3x3_BottleneckConv_in->360->1024_S1, Identity)\n",
      "(3x3_BottleneckConv_in->360->1024_S1, Identity)\n",
      "(3x3_BottleneckConv_in->360->1024_S1, Identity)\n",
      "(3x3_BottleneckConv_in->360->1024_S1, Identity)\n",
      "(3x3_BottleneckConv_in->720->2048_S2, avgpool_conv)\n",
      "(3x3_BottleneckConv_in->720->2048_S1, Identity)\n",
      "(3x3_BottleneckConv_in->720->2048_S1, Identity)\n",
      "(3x3_BottleneckConv_in->512->2048_S1, Identity)\n",
      "MyGlobalAvgPool2d(keep_dim=False)\n",
      "DyLinear(2048, 1000)\n",
      "FLOPS 2015.449088 workingmem 392.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Hyper-parameters for the evolutionary search process\n",
    "    You can modify these hyper-parameters to see how they influence the final ImageNet accuracy of the search sub-net.\n",
    "\"\"\"\n",
    "FLOPs_constraint = 5000  # MFLOPs\n",
    "workingmem_constraint = 400# KB\n",
    "P = 100  # The size of population in each generation\n",
    "N = 200  # How many generations of population to be searched\n",
    "r = 0.25  # The ratio of networks that are used as parents for next generation\n",
    "params = {\n",
    "    #'constraint_type': target_hardware, # Let's do FLOPs-constrained search\n",
    "    #'efficiency_constraint': FLOPs_constraint,\n",
    "    'mutate_prob': 0.1, # The probability of mutation in evolutionary search\n",
    "    'mutation_ratio': 0.5, # The ratio of networks that are generated through mutation in generation n >= 2.\n",
    "    'efficiency_predictor': efficiency_predictor, # To use a predefined efficiency predictor.\n",
    "    'accuracy_predictor': acc_predictor, # To use a predefined accuracy_predictor predictor.\n",
    "    'memory_predictor': memory_predictor_baseline, # To use a predefined working memory predictor\n",
    "    'population_size': P,\n",
    "    'max_time_budget': N,\n",
    "    'parent_ratio': r,\n",
    "}\n",
    "\n",
    "# build the evolution finder\n",
    "finder = EvolutionFinder(**params)\n",
    "\n",
    "# start searching\n",
    "result_lis = []\n",
    "st = time.time()\n",
    "best_valids, best_info = finder.run_evolution_search(FLOPs_constraint, workingmem_constraint, verbose=True)\n",
    "result_lis.append(best_info)\n",
    "ed = time.time()\n",
    "print('Found best architecture with FLOPS <= %.2f M and working mem <= %.2f in %.2f seconds! '\n",
    "      'It achieves %.2f%s predicted accuracy ' %\n",
    "      (FLOPs_constraint, workingmem_constraint, ed-st, best_info[0] * 100, '%'))\n",
    "\n",
    "# visualize the architecture of the searched sub-net\n",
    "_, net_config, FLOPS, workingmem = best_info\n",
    "ofa_network.set_active_subnet(w=net_config['w'], d=net_config['d'], e=net_config['e'])\n",
    "print('Architecture of the searched sub-net:')\n",
    "print(ofa_network.module_str)\n",
    "print('FLOPS',FLOPS,'workingmem', workingmem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate random population...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching with constraint (5000,400):   0%|          | 0/200 [00:00<?, ?it/s, acc=0.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Evolution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching with constraint (5000,400): 100%|██████████| 200/200 [00:49<00:00,  4.03it/s, acc=0.802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found best architecture with FLOPS <= 5000.00 M and working mem <= 400.00 in 53.45 seconds! It achieves 80.24% predicted accuracy \n",
      "Architecture of the searched sub-net:\n",
      "DyConv(O24, K3, S2)\n",
      "(DyConv(O24, K3, S1), Identity)\n",
      "DyConv(O64, K3, S1)\n",
      "max_pooling(ks=3, stride=2)\n",
      "(3x3_BottleneckConv_in->56->168_S1, avgpool_conv)\n",
      "(3x3_BottleneckConv_in->56->168_S1, Identity)\n",
      "(3x3_BottleneckConv_in->56->168_S1, Identity)\n",
      "(3x3_BottleneckConv_in->56->168_S1, Identity)\n",
      "(3x3_BottleneckConv_in->176->512_S2, avgpool_conv)\n",
      "(3x3_BottleneckConv_in->176->512_S1, Identity)\n",
      "(3x3_BottleneckConv_in->128->512_S1, Identity)\n",
      "(3x3_BottleneckConv_in->176->512_S1, Identity)\n",
      "(3x3_BottleneckConv_in->360->1024_S2, avgpool_conv)\n",
      "(3x3_BottleneckConv_in->360->1024_S1, Identity)\n",
      "(3x3_BottleneckConv_in->360->1024_S1, Identity)\n",
      "(3x3_BottleneckConv_in->360->1024_S1, Identity)\n",
      "(3x3_BottleneckConv_in->360->1024_S1, Identity)\n",
      "(3x3_BottleneckConv_in->360->1024_S1, Identity)\n",
      "(3x3_BottleneckConv_in->720->2048_S2, avgpool_conv)\n",
      "(3x3_BottleneckConv_in->720->2048_S1, Identity)\n",
      "(3x3_BottleneckConv_in->720->2048_S1, Identity)\n",
      "(3x3_BottleneckConv_in->512->2048_S1, Identity)\n",
      "MyGlobalAvgPool2d(keep_dim=False)\n",
      "DyLinear(2048, 1000)\n",
      "FLOPS 2015.449088 workingmem 344.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "FLOPs_constraint = 5000  # MFLOPs\n",
    "workingmem_constraint = 400 # KB\n",
    "P = 100  # The size of population in each generation\n",
    "N = 200  # How many generations of population to be searched\n",
    "r = 0.25  # The ratio of networks that are used as parents for next generation\n",
    "params = {\n",
    "    #'constraint_type': target_hardware, # Let's do FLOPs-constrained search\n",
    "    #'efficiency_constraint': FLOPs_constraint,\n",
    "    'mutate_prob': 0.1, # The probability of mutation in evolutionary search\n",
    "    'mutation_ratio': 0.5, # The ratio of networks that are generated through mutation in generation n >= 2.\n",
    "    'efficiency_predictor': efficiency_predictor, # To use a predefined efficiency predictor.\n",
    "    'accuracy_predictor': acc_predictor, # To use a predefined accuracy_predictor predictor.\n",
    "    'memory_predictor': memory_predictor_ideal, # To use a predefined working memory predictor\n",
    "    'population_size': P,\n",
    "    'max_time_budget': N,\n",
    "    'parent_ratio': r,\n",
    "}\n",
    "\n",
    "# build the evolution finder\n",
    "finder = EvolutionFinder(**params)\n",
    "\n",
    "# start searching\n",
    "result_lis = []\n",
    "st = time.time()\n",
    "best_valids, best_info = finder.run_evolution_search(FLOPs_constraint, workingmem_constraint, verbose=True)\n",
    "result_lis.append(best_info)\n",
    "ed = time.time()\n",
    "print('Found best architecture with FLOPS <= %.2f M and working mem <= %.2f in %.2f seconds! '\n",
    "      'It achieves %.2f%s predicted accuracy ' %\n",
    "      (FLOPs_constraint, workingmem_constraint, ed-st, best_info[0] * 100, '%'))\n",
    "\n",
    "# visualize the architecture of the searched sub-net\n",
    "_, net_config, FLOPS, workingmem = best_info\n",
    "ofa_network.set_active_subnet(w=net_config['w'], d=net_config['d'], e=net_config['e'])\n",
    "print('Architecture of the searched sub-net:')\n",
    "print(ofa_network.module_str)\n",
    "print('FLOPS',FLOPS,'workingmem', workingmem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate random population...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching with constraint (2000,370):   0%|          | 0/200 [00:00<?, ?it/s, acc=0.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Evolution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching with constraint (2000,370):  84%|████████▍ | 168/200 [1:15:29<14:06, 26.45s/it, acc=0.802]"
     ]
    }
   ],
   "source": [
    "FLOPs_constraint = 2000  # MFLOPs\n",
    "workingmem_constraint = 370 # KB\n",
    "P = 100  # The size of population in each generation\n",
    "N = 200  # How many generations of population to be searched\n",
    "r = 0.25  # The ratio of networks that are used as parents for next generation\n",
    "params = {\n",
    "    #'constraint_type': target_hardware, # Let's do FLOPs-constrained search\n",
    "    #'efficiency_constraint': FLOPs_constraint,\n",
    "    'mutate_prob': 0.1, # The probability of mutation in evolutionary search\n",
    "    'mutation_ratio': 0.5, # The ratio of networks that are generated through mutation in generation n >= 2.\n",
    "    'efficiency_predictor': efficiency_predictor, # To use a predefined efficiency predictor.\n",
    "    'accuracy_predictor': acc_predictor, # To use a predefined accuracy_predictor predictor.\n",
    "    'memory_predictor': memory_predictor_self, # To use a predefined working memory predictor\n",
    "    'population_size': P,\n",
    "    'max_time_budget': N,\n",
    "    'parent_ratio': r,\n",
    "}\n",
    "\n",
    "# build the evolution finder\n",
    "finder = EvolutionFinder(**params)\n",
    "\n",
    "# start searching\n",
    "result_lis = []\n",
    "st = time.time()\n",
    "best_valids, best_info = finder.run_evolution_search(FLOPs_constraint, workingmem_constraint, verbose=True)\n",
    "result_lis.append(best_info)\n",
    "ed = time.time()\n",
    "print('Found best architecture with FLOPS <= %.2f M and working mem <= %.2f in %.2f seconds! '\n",
    "      'It achieves %.2f%s predicted accuracy ' %\n",
    "      (FLOPs_constraint, workingmem_constraint, ed-st, best_info[0] * 100, '%'))\n",
    "\n",
    "# visualize the architecture of the searched sub-net\n",
    "_, net_config, FLOPS, workingmem = best_info\n",
    "ofa_network.set_active_subnet(w=net_config['w'], d=net_config['d'], e=net_config['e'])\n",
    "print('Architecture of the searched sub-net:')\n",
    "print(ofa_network.module_str)\n",
    "print('FLOPS',FLOPS,'workingmem', workingmem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "dev_env",
   "display_name": "dev_env",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "interpreter": {
   "hash": "c7481cae3887e79a1c2e6c0eed6abfa1a5903a6874eaf9fc46b8afa5c42f627c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}